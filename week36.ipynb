{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGSiMGOCCGzgLtMxNWp6Jr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OsYqwcCDl41",
        "outputId": "7429cc45-1cd5-4dba-eab2-567f8212f480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.66.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2023.7.22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install bpemb\n",
        "\n",
        "import nltk\n",
        "from bpemb import BPEmb\n",
        "import string\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "train_set = dataset[\"train\"]\n",
        "validation_set = dataset[\"validation\"]\n",
        "\n",
        "def get_answer_start(row):\n",
        "  return row[\"annotations\"][\"answer_start\"][0]\n",
        "\n",
        "def get_answer(row):\n",
        "  return row[\"annotations\"][\"answer_text\"][0]\n",
        "\n",
        "def get_document(row):\n",
        "  return row[\"document_plaintext\"]\n",
        "\n",
        "def get_question(row):\n",
        "  return row[\"question_text\"]\n",
        "\n",
        "def oracle(answer, document):\n",
        "  return answer != \"\" and answer in document\n",
        "\n",
        "def get_language(dataset, lang):\n",
        "  return [row for row in dataset if row['language'] == lang]\n",
        "\n",
        "train_arabic = get_language(train_set, \"arabic\")\n",
        "val_arabic = get_language(validation_set, \"arabic\")\n",
        "\n",
        "train_bengali = get_language(train_set, \"bengali\")\n",
        "val_bengali = get_language(validation_set, \"bengali\")\n",
        "\n",
        "train_indonesian = get_language(train_set, \"indonesian\")\n",
        "val_indonesian = get_language(validation_set, \"indonesian\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ratio_string(train, val):\n",
        "  val_ratio = round(len(val)/len(train)*100)\n",
        "  train_ratio = 100-val_ratio\n",
        "  return f\"{train_ratio} / {val_ratio}\"\n",
        "\n",
        "def answerable_ratio(ds):\n",
        "  answerable = round(sum([1 for row in ds if get_answer_start(row) == -1])/len(ds)*100)\n",
        "  nonansw = 100-answerable\n",
        "  return f\"{answerable} / {nonansw}\"\n",
        "\n",
        "def avg_column_length(ds, getter):\n",
        "  lengths = [len(getter(row)) for row in ds]\n",
        "  return sum(lengths)/len(lengths)\n",
        "\n",
        "print(f\"\"\"\n",
        "Dataset features:\n",
        "\n",
        "{train_set.column_names}\n",
        "\n",
        "Dataset sizes:\n",
        "\n",
        "(Arabic) Training set:                                          {len(train_arabic)}\n",
        "(Arabic) Validation set:                                        {len(val_arabic)}\n",
        "(Arabic) Ratio (Training/Val):                                  {ratio_string(train_arabic, val_arabic)}\n",
        "(Arabic) Training balance (Answerable / Not answerable):        {answerable_ratio(train_arabic)}\n",
        "(Arabic) Validation balance (Answerable / Not answerable):      {answerable_ratio(val_arabic)}\n",
        "(Arabic) Avg Document string lengths (Train/Val):               {avg_column_length(train_arabic, get_document)} / {avg_column_length(val_arabic, get_document)}\n",
        "(Arabic) Avg Question string lengths (Train/Val):               {avg_column_length(train_arabic, get_question)} / {avg_column_length(val_arabic, get_question)}\n",
        "\n",
        "(Bengali) Training set:                                         {len(train_bengali)}\n",
        "(Bengali) Validation set:                                       {len(val_bengali)}\n",
        "(Bengali) Ratio (Training/Val):                                 {ratio_string(train_bengali, val_bengali)}\n",
        "(Bengali) Training balance (Answerable / Not answerable):       {answerable_ratio(train_bengali)}\n",
        "(Bengali) Validation balance (Answerable / Not answerable):     {answerable_ratio(val_bengali)}\n",
        "(Bengali) Avg Document string lengths (Train/Val):              {avg_column_length(train_bengali, get_document)} / {avg_column_length(val_bengali, get_document)}\n",
        "(Bengali) Avg Question string lengths (Train/Val):              {avg_column_length(train_bengali, get_question)} / {avg_column_length(val_bengali, get_question)}\n",
        "\n",
        "(Indonesian) Training set:                                      {len(train_indonesian)}\n",
        "(Indonesian) Validation set:                                    {len(val_indonesian)}\n",
        "(Indonesian) Ratio (Training/Val):                              {ratio_string(train_indonesian, val_indonesian)}\n",
        "(Indonesian) Training balance (Answerable / Not answerable):    {answerable_ratio(train_indonesian)}\n",
        "(Indonesian) Validation balance (Answerable / Not answerable):  {answerable_ratio(val_indonesian)}\n",
        "(Indonesian) Avg Document string lengths (Train/Val):           {avg_column_length(train_indonesian, get_document)} / {avg_column_length(val_indonesian, get_document)}\n",
        "(Indonesian) Avg Question string lengths (Train/Val):           {avg_column_length(train_indonesian, get_question)} / {avg_column_length(val_indonesian, get_question)}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVs-9TQ_EC-v",
        "outputId": "93e2cd10-a73a-438b-8a94-deb306bfd2fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset features:\n",
            "\n",
            "['question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url']\n",
            "\n",
            "Dataset sizes:\n",
            "\n",
            "(Arabic) Training set:                                          29598\n",
            "(Arabic) Validation set:                                        1902\n",
            "(Arabic) Ratio (Training/Val):                                  94 / 6\n",
            "(Arabic) Training balance (Answerable / Not answerable):        50 / 50\n",
            "(Arabic) Validation balance (Answerable / Not answerable):      50 / 50\n",
            "(Arabic) Avg Document string lengths (Train/Val):               474.58760727076157 / 423.8958990536278\n",
            "(Arabic) Avg Question string lengths (Train/Val):               29.355665923373202 / 29.31335436382755\n",
            "\n",
            "(Bengali) Training set:                                         4779\n",
            "(Bengali) Validation set:                                       224\n",
            "(Bengali) Ratio (Training/Val):                                 95 / 5\n",
            "(Bengali) Training balance (Answerable / Not answerable):       50 / 50\n",
            "(Bengali) Validation balance (Answerable / Not answerable):     50 / 50\n",
            "(Bengali) Avg Document string lengths (Train/Val):              554.5662272441933 / 562.0535714285714\n",
            "(Bengali) Avg Question string lengths (Train/Val):              47.54969658924461 / 51.69642857142857\n",
            "\n",
            "(Indonesian) Training set:                                      11394\n",
            "(Indonesian) Validation set:                                    1191\n",
            "(Indonesian) Ratio (Training/Val):                              90 / 10\n",
            "(Indonesian) Training balance (Answerable / Not answerable):    50 / 50\n",
            "(Indonesian) Validation balance (Answerable / Not answerable):  50 / 50\n",
            "(Indonesian) Avg Document string lengths (Train/Val):           529.9210110584518 / 506.32409739714524\n",
            "(Indonesian) Avg Question string lengths (Train/Val):           36.95664384763911 / 39.130982367758186\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(dataset, column):\n",
        "  bag = {}\n",
        "  for row in dataset:\n",
        "    tokens = nltk.word_tokenize(row[column])\n",
        "\n",
        "    for token in tokens:\n",
        "\n",
        "      if not token in bag:\n",
        "        bag[token] = 0\n",
        "\n",
        "      bag[token] += 1\n",
        "      #print(bag)\n",
        "  return sorted(bag.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "def sort_bag(bag):\n",
        "  return sorted(bag.items(), key=lambda item: item[1], reverse=True)"
      ],
      "metadata": {
        "id": "ssewoH3bEFNl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_doc_bow = bag_of_words(train_arabic, \"document_plaintext\")\n",
        "arabic_question_bow = bag_of_words(train_arabic, \"question_text\")\n",
        "\n",
        "bengali_doc_bow = bag_of_words(train_bengali, \"document_plaintext\")\n",
        "bengali_question_bow = bag_of_words(train_bengali, \"question_text\")\n",
        "\n",
        "indonesian_doc_bow = bag_of_words(train_indonesian, \"document_plaintext\")\n",
        "indonesian_question_bow = bag_of_words(train_indonesian, \"question_text\")"
      ],
      "metadata": {
        "id": "Z5Z_w_twEFon"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\n",
        "\n",
        "Most common words:\n",
        "\n",
        "(Arabic) Documents: {arabic_doc_bow[0:5]}\n",
        "(Arabic) Questions: {arabic_question_bow[0:5]}\n",
        "\n",
        "(Bengali) Documents: {bengali_doc_bow[0:5]}\n",
        "(Bengali) Questions: {bengali_question_bow[0:5]}\n",
        "\n",
        "(Indonesian) Documents: {indonesian_doc_bow[0:5]}\n",
        "(Indonesian) Questions: {indonesian_question_bow[0:5]}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhM8oZcoEGsQ",
        "outputId": "f229c582-bad5-4acf-bb9e-ab0d7c1637ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Most common words:\n",
            "\n",
            "(Arabic) Documents: [('في', 89705), ('.', 88299), ('من', 61719), ('[', 38120), (']', 38119)]\n",
            "(Arabic) Questions: [('؟', 10061), ('ما', 7451), ('متى', 7130), ('هو', 6760), ('من', 6309)]\n",
            "\n",
            "(Bengali) Documents: [(',', 12184), (']', 7123), ('[', 7120), ('ও', 5195), ('এবং', 5102)]\n",
            "(Bengali) Questions: [('?', 4777), ('কী', 940), ('নাম', 837), ('কত', 802), ('হয়', 800)]\n",
            "\n",
            "(Indonesian) Documents: [(',', 54165), ('.', 43063), ('yang', 24077), ('dan', 23741), ('di', 16604)]\n",
            "(Indonesian) Questions: [('?', 11368), ('yang', 1814), ('Kapan', 1811), ('Apa', 1633), ('Apakah', 1227)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ratio(question, document, stop_words):\n",
        "  tokens = nltk.word_tokenize(question)\n",
        "  count = 0\n",
        "  stripped_tokens = set(tokens) - stop_words\n",
        "  for token in stripped_tokens:\n",
        "    if token in document:\n",
        "      count += 1\n",
        "  return count/len(stripped_tokens)\n",
        "\n",
        "\n",
        "def avg(lst):\n",
        "  return sum(lst)/len(lst)\n",
        "\n",
        "def get_average_ratios(training, stop_words):\n",
        "  answerable_ratios = []\n",
        "  nonanswerable_ratios = []\n",
        "  for row in training:\n",
        "    ratio = get_ratio(get_question(row), get_document(row), stop_words)\n",
        "    lst = answerable_ratios if oracle(get_answer(row), get_document(row)) else nonanswerable_ratios\n",
        "    lst.append(ratio)\n",
        "  return avg(answerable_ratios), avg(nonanswerable_ratios)\n",
        "\n",
        "class NaiveModel:\n",
        "  def __init__(self, stop_words):\n",
        "    self.stop_words = stop_words\n",
        "    self.ratio = -1\n",
        "\n",
        "  def train(self, training):\n",
        "    answerable_ratio, nonanswerable_ratio = get_average_ratios(training, self.stop_words)\n",
        "    self.ratio = (answerable_ratio + nonanswerable_ratio)/2\n",
        "\n",
        "  def classify(self, question, document):\n",
        "    return get_ratio(question, document, self.stop_words) > self.ratio\n",
        "\n",
        "def evaluate(validation, model):\n",
        "  res = [int(oracle(get_answer(row), get_document(row)) == model.classify(get_question(row), get_document(row))) for row in validation]\n",
        "  acc = avg(res)\n",
        "\n",
        "  ### Manual generation of confusion matrix for scores like Balanced Accuray and F-score\n",
        "  #tp, fp, tn, fn = 0, 0, 0, 0\n",
        "  #for row in validation:\n",
        "  #  gt = oracle(get_answer(row), get_document(row))\n",
        "  #  cl = model.classify(get_question(row), get_document(row))\n",
        "  #  if (cl):\n",
        "  #    if (gt):\n",
        "  #      tp += 1\n",
        "  #    else:\n",
        "  #      fp += 1\n",
        "  #  else:\n",
        "  #    if (gt):\n",
        "  #      fn += 1\n",
        "  #    else:\n",
        "  #      tn += 1\n",
        "  #tpr = tp / (tp + fn)\n",
        "  #tnr = tn / (tn + fp)\n",
        "  #ba = (tpr + tnr) / 2\n",
        "\n",
        "  print(f\"Accuracy: {round(acc*100, 4)}%\\n\")"
      ],
      "metadata": {
        "id": "9yck_8wREH0z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "arabic_stop_words = set(stopwords.words('indonesian')) | set(string.punctuation) | set(\"؟\")\n",
        "bengali_stop_words = set(stopwords.words('bengali')) | set(string.punctuation)\n",
        "indonesian_stop_words = set(stopwords.words('indonesian')) | set(string.punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9uVu71GEK-c",
        "outputId": "18b4c6f7-60cb-468a-abc2-f8fd39318656"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating arabic:\")\n",
        "\n",
        "arabic_model = NaiveModel(arabic_stop_words)\n",
        "arabic_model.train(train_arabic)\n",
        "evaluate(val_arabic, arabic_model)\n",
        "\n",
        "print(\"Evaluating bengali:\")\n",
        "\n",
        "bengali_model = NaiveModel(bengali_stop_words)\n",
        "bengali_model.train(train_bengali)\n",
        "evaluate(val_bengali, bengali_model)\n",
        "\n",
        "print(\"Evaluating indonesian:\")\n",
        "\n",
        "indonesian_model = NaiveModel(indonesian_stop_words)\n",
        "indonesian_model.train(train_indonesian)\n",
        "evaluate(val_indonesian, indonesian_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCqm2vSZESVr",
        "outputId": "64f08f79-617a-4e3e-f46b-c6219bf3e5de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating arabic:\n",
            "Accuracy: 71.6614%\n",
            "\n",
            "Evaluating bengali:\n",
            "Accuracy: 72.3214%\n",
            "\n",
            "Evaluating indonesian:\n",
            "Accuracy: 71.2007%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mF2StlujaEzS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}